{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59441052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Cell 1: Core Libraries\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69507203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15905e585d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üå± Cell 2: Reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80d45c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ† Using image folder: D:\\Documents\\CODE_WITH_ERICADESHH\\GitHub\\HarvestGuard\\data\\preprocessed\\fertilizer\\YaraMila\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Cell 3: Project Paths\n",
    "PROJECT_ROOT = pathlib.Path(os.getcwd()).resolve().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"preprocessed\" / \"fertilizer\" / \"YaraMila\"\n",
    "IMAGE_DIR = str(DATA_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (128, 128)\n",
    "NUM_EPISODES = 5\n",
    "MAX_STEPS = 20\n",
    "LR = 1e-4\n",
    "THRESHOLD = 0.0148  # ‚Üê you can inject from settings.yaml if preferred\n",
    "\n",
    "print(f\"üõ† Using image folder: {IMAGE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e2dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Cell 4: Load Trained Autoencoder with Scoring Logic\n",
    "\n",
    "import sys\n",
    "\n",
    "# Add project root to sys.path so we can import from models/\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from models.autoencoder import ConvAutoencoder\n",
    "\n",
    "# Load the autoencoder\n",
    "autoencoder = ConvAutoencoder().to(DEVICE)\n",
    "autoencoder_path = PROJECT_ROOT / \"models\" / \"checkpoints\" / \"autoencoder.pth\"\n",
    "autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=DEVICE))\n",
    "autoencoder.eval()\n",
    "\n",
    "# Scoring function (MSE)\n",
    "def anomaly_score(img_tensor):\n",
    "    with torch.no_grad():\n",
    "        recon = autoencoder(img_tensor)\n",
    "        return torch.nn.functional.mse_loss(recon, img_tensor).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf499890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Cell 5: Enhanced RL Environment with anomaly scoring\n",
    "\n",
    "class SmarterImageScanEnv:\n",
    "    def __init__(self, image_dir, autoencoder_model):\n",
    "        self.image_dir = image_dir\n",
    "        self.autoencoder = autoencoder_model\n",
    "        self.index = 0\n",
    "\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise FileNotFoundError(f\"‚ùå Directory not found: {image_dir}\")\n",
    "\n",
    "        self.images = sorted([\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(f\"‚ùå No valid image files in: {image_dir}\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.index >= len(self.images):\n",
    "            return None\n",
    "        path = os.path.join(self.image_dir, self.images[self.index])\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Action: 0 = mark as REAL, 1 = mark as FAKE\"\"\"\n",
    "        img_tensor = self._get_obs()\n",
    "\n",
    "        # Compute anomaly score (lower = more real)\n",
    "        score = anomaly_score(img_tensor)\n",
    "        threshold = 0.0148  # Ideally this should be dynamic/configurable\n",
    "\n",
    "        is_anomaly = score > threshold\n",
    "        user_feedback = 1 if is_anomaly else 0  # Simulated\n",
    "\n",
    "        # Reward logic\n",
    "        if action == 1 and is_anomaly:\n",
    "            reward = +1  # Correctly flagged fake\n",
    "        elif action == 0 and not is_anomaly:\n",
    "            reward = +1  # Correctly identified real\n",
    "        else:\n",
    "            reward = -1  # Wrong classification\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.images)\n",
    "        return self._get_obs(), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83adfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèãÔ∏è Cell 6: Setup training with smarter environment\n",
    "# ü§ñ Cell X: Define Enhanced Agent\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 128, 128), num_actions=2):\n",
    "        super(Agent, self).__init__()\n",
    "        c, h, w = input_shape\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c, 16, kernel_size=3, stride=2, padding=1),  # (B, 16, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # (B, 32, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 32 * 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "env = SmarterImageScanEnv(IMAGE_DIR, autoencoder)\n",
    "model = Agent().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "gamma = 0.95  # Discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "217ed636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Episode 1/5 | Total Reward: 20\n",
      "üìà Episode 2/5 | Total Reward: 20\n",
      "üìà Episode 3/5 | Total Reward: 20\n",
      "üìà Episode 4/5 | Total Reward: 20\n",
      "üìà Episode 5/5 | Total Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# üîÅ Cell 7: Train Agent with Smart Reward Logic\n",
    "from collections import deque\n",
    "\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        q_values = model(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Estimate future reward (Q-learning)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(next_state) if next_state is not None else torch.zeros_like(q_values)\n",
    "            max_next_q = torch.max(next_q_values)\n",
    "            target_q = q_values.clone()\n",
    "            target_q[0][action] = reward + gamma * max_next_q\n",
    "\n",
    "        # Loss & Backprop\n",
    "        loss = loss_fn(q_values, target_q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done or next_state is None:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"üìà Episode {episode+1}/{NUM_EPISODES} | Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa30fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smarter RL agent saved to: D:\\Documents\\CODE_WITH_ERICADESHH\\GitHub\\HarvestGuard\\models\\checkpoints\\rl_agent_smart.pth\n"
     ]
    }
   ],
   "source": [
    "# üíæ Cell 8: Save the trained model\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"models\" / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_path = CHECKPOINT_DIR / \"rl_agent_smart.pth\"\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"‚úÖ Smarter RL agent saved to: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e23279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Agent Evaluation Run:\n",
      "‚úÖ Total Reward Collected by Smart Agent: 30\n"
     ]
    }
   ],
   "source": [
    "# üß† Cell 9: Evaluate Smarter Agent\n",
    "print(\"üîç Agent Evaluation Run:\")\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done and state is not None:\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"‚úÖ Total Reward Collected by Smart Agent: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fed0d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # For PDF export\n",
    "from IPython.display import display, HTML  # For inline display or HTML export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bee1fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéûÔ∏è RL Agent Visual Inference Demo:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1772\\1071161886.py:53: UserWarning: Glyph 9989 (\\N{WHITE HEAVY CHECK MARK}) missing from font(s) DejaVu Sans.\n",
      "  pdf_report.savefig(fig)  # Save this fig to PDF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ PDF report saved: rl_agent_report.pdf\n",
      "üèÅ Demo finished ‚Äî Total Reward Collected: 30\n"
     ]
    }
   ],
   "source": [
    "# üìä RL Agent Demo with Visual Output\n",
    "print(\"üéûÔ∏è RL Agent Visual Inference Demo:\")\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "state = env._get_obs()\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "results = []\n",
    "figures = []\n",
    "\n",
    "pdf_report = PdfPages(\"rl_agent_report.pdf\")  # üìÑ PDF init\n",
    "\n",
    "while not done and state is not None:\n",
    "    with torch.no_grad():\n",
    "        # ‚úÖ Fix shape bug\n",
    "        if state.dim() == 3:\n",
    "            state_input = state.unsqueeze(0)  # (1, 3, 128, 128)\n",
    "        else:\n",
    "            state_input = state  # Already batched\n",
    "\n",
    "        action_values = model(state_input)\n",
    "        action = torch.argmax(action_values).item()\n",
    "\n",
    "        # üéØ Inference from autoencoder\n",
    "        recon = autoencoder(state_input)\n",
    "        error = torch.mean((recon - state_input) ** 2).item()\n",
    "\n",
    "        # üîç Result Label\n",
    "        result = \"‚úÖ Accept\" if action == 0 else \"üö® Flag\"\n",
    "\n",
    "        # üìä Store this step\n",
    "        results.append({\n",
    "            \"step\": step,\n",
    "            \"image_path\": env.images[env.index],\n",
    "            \"action\": result,\n",
    "            \"error\": round(error, 5)\n",
    "        })\n",
    "\n",
    "        # üñº Side-by-side view\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs[0].imshow(TF.to_pil_image(state.squeeze().cpu()))\n",
    "        axs[0].set_title(\"Original\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(TF.to_pil_image(recon.squeeze().cpu()))\n",
    "        axs[1].set_title(f\"Reconstruction\\nError: {round(error, 5)}\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        fig.suptitle(f\"Step {step} | {result}\", fontsize=12)\n",
    "        figures.append(fig)\n",
    "        pdf_report.savefig(fig)  # Save this fig to PDF\n",
    "        plt.close(fig)\n",
    "\n",
    "        # üîÅ Next step\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "pdf_report.close()\n",
    "print(f\"üìÑ PDF report saved: rl_agent_report.pdf\")\n",
    "print(f\"üèÅ Demo finished ‚Äî Total Reward Collected: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa0e1cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>RL Agent Decision Log</h3>\n",
       "<table border='1' style='border-collapse: collapse;'>\n",
       "<tr><th>Step</th><th>Image</th><th>Action</th><th>Recon Error</th></tr>\n",
       "<tr><td>0</td><td>000001.jpg</td><td>‚úÖ Accept</td><td>0.00663</td></tr><tr><td>1</td><td>000002.jpg</td><td>‚úÖ Accept</td><td>0.007</td></tr><tr><td>2</td><td>000003.jpg</td><td>‚úÖ Accept</td><td>0.00787</td></tr><tr><td>3</td><td>000004.jpg</td><td>‚úÖ Accept</td><td>0.00766</td></tr><tr><td>4</td><td>000005.jpg</td><td>‚úÖ Accept</td><td>0.00251</td></tr><tr><td>5</td><td>000006.jpg</td><td>‚úÖ Accept</td><td>0.00703</td></tr><tr><td>6</td><td>000007.jpg</td><td>‚úÖ Accept</td><td>0.00182</td></tr><tr><td>7</td><td>000008.jpg</td><td>‚úÖ Accept</td><td>0.00243</td></tr><tr><td>8</td><td>000009.jpg</td><td>‚úÖ Accept</td><td>0.00646</td></tr><tr><td>9</td><td>000010.jpg</td><td>‚úÖ Accept</td><td>0.0029</td></tr><tr><td>10</td><td>000011.jpg</td><td>‚úÖ Accept</td><td>0.00138</td></tr><tr><td>11</td><td>000012.jpg</td><td>‚úÖ Accept</td><td>0.00311</td></tr><tr><td>12</td><td>000013.jpg</td><td>‚úÖ Accept</td><td>0.0017</td></tr><tr><td>13</td><td>000014.jpg</td><td>‚úÖ Accept</td><td>0.00786</td></tr><tr><td>14</td><td>000015.jpg</td><td>‚úÖ Accept</td><td>0.01126</td></tr><tr><td>15</td><td>000016.jpg</td><td>‚úÖ Accept</td><td>0.00714</td></tr><tr><td>16</td><td>000017.jpg</td><td>‚úÖ Accept</td><td>0.01007</td></tr><tr><td>17</td><td>000018.jpg</td><td>‚úÖ Accept</td><td>0.00397</td></tr><tr><td>18</td><td>000019.jpg</td><td>‚úÖ Accept</td><td>0.01143</td></tr><tr><td>19</td><td>000020.jpg</td><td>‚úÖ Accept</td><td>0.00549</td></tr><tr><td>20</td><td>000022.jpg</td><td>‚úÖ Accept</td><td>0.00199</td></tr><tr><td>21</td><td>000023.jpg</td><td>‚úÖ Accept</td><td>0.00444</td></tr><tr><td>22</td><td>000024.jpg</td><td>‚úÖ Accept</td><td>0.0048</td></tr><tr><td>23</td><td>000025.jpg</td><td>‚úÖ Accept</td><td>0.01219</td></tr><tr><td>24</td><td>000026.jpg</td><td>‚úÖ Accept</td><td>0.0075</td></tr><tr><td>25</td><td>000027.jpg</td><td>‚úÖ Accept</td><td>0.00527</td></tr><tr><td>26</td><td>000028.jpg</td><td>‚úÖ Accept</td><td>0.01259</td></tr><tr><td>27</td><td>000029.jpg</td><td>‚úÖ Accept</td><td>0.00747</td></tr><tr><td>28</td><td>000030.jpg</td><td>‚úÖ Accept</td><td>0.00505</td></tr><tr><td>29</td><td>000031.jpg</td><td>‚úÖ Accept</td><td>0.00527</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HTML log saved: rl_agent_results.html\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Save results as HTML (inline + file)\n",
    "html_rows = \"\".join([\n",
    "    f\"<tr><td>{r['step']}</td><td>{r['image_path']}</td><td>{r['action']}</td><td>{r['error']}</td></tr>\"\n",
    "    for r in results\n",
    "])\n",
    "html_table = f\"\"\"\n",
    "<h3>RL Agent Decision Log</h3>\n",
    "<table border='1' style='border-collapse: collapse;'>\n",
    "<tr><th>Step</th><th>Image</th><th>Action</th><th>Recon Error</th></tr>\n",
    "{html_rows}\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "# Inline display\n",
    "display(HTML(html_table))\n",
    "\n",
    "# ‚úÖ Save HTML using utf-8 to fix emoji error\n",
    "with open(\"rl_agent_results.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_table)\n",
    "\n",
    "print(\"‚úÖ HTML log saved: rl_agent_results.html\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
